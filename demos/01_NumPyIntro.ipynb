{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naica922/uek_259_MachieneLearning/blob/main/demos/01_NumPyIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4r2z30vJSbA"
      },
      "source": [
        "# NumPy UltraQuick Tutorial\n",
        "\n",
        "NumPy is a Python library for creating and manipulating vectors and matrices. This Colab is not an exhaustive tutorial on NumPy.  Rather, this Colab teaches you just enough to use NumPy in the Colab exercises of Machine Learning Crash Course.\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LuWidme/uk259/blob/main/demos/NumPy%20Intro.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO47lN3aDOAv"
      },
      "source": [
        "## About Jupyter\n",
        "\n",
        "Jupyter Notebooks consist of two kinds of components:\n",
        "\n",
        "  * **Text cells**, which contain explanations. You are currently reading a text cell.\n",
        "  * **Code cells**, which contain Python code for you to run. Code cells have a light gray background.\n",
        "\n",
        "You *read* the text cells and *run* the code cells.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ-pvdPymocS"
      },
      "source": [
        "### Running code cells\n",
        "\n",
        "You must run code cells in order. In other words, you may only run a code cell once all the code cells preceding it have already been run.\n",
        "\n",
        "To run a code cell:\n",
        "\n",
        "  1. Place the cursor anywhere inside the [ ] area at the top left of a code cell. The area inside the [ ] will display an arrow.\n",
        "  2. Click the arrow.\n",
        "\n",
        "Alternatively, you may invoke **Run->Run all**."
      ]
    },
    {
      "metadata": {
        "id": "FtE0dSO4_sEx"
      },
      "cell_type": "markdown",
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9GhD7Fsmoqw"
      },
      "source": [
        "### If you see errors...\n",
        "\n",
        "The most common reasons for seeing code cell errors are as follows:\n",
        "\n",
        "  * You didn't run *all* of the code cells preceding the current code cell.\n",
        "  * If the code cell is labeled as a **Task**, then:\n",
        "    *  You haven't yet written the code that implements the task.\n",
        "    *  You did write the code, but the code contained errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9RWewwFwX6"
      },
      "source": [
        "## Import NumPy module\n",
        "\n",
        "Run the following code cell to import the NumPy module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guvPzSWYJGZ4",
        "jupyter": {
          "is_executing": true
        }
      },
      "source": [
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cT9fXS_JUpa"
      },
      "source": [
        "## Populate arrays with specific numbers\n",
        "\n",
        "Call `np.array` to create a NumPy matrix with your own hand-picked values. For example, the following call to `np.array` creates an 8-element vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxJR5xKpJbB3"
      },
      "outputs": [],
      "source": [
        "one_dimensional_array = np.array([1.2, 2.4, 3.5, 4.7, 6.1, 7.2, 8.3, 9.5])\n",
        "print(one_dimensional_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKywqhLTbR1M"
      },
      "source": [
        "You can also use `np.array` to create a two-dimensional matrix. To create a two-dimensional matrix, specify an extra layer of square brackets. For example, the following call creates a 3x2 matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_veGj18eMCDu"
      },
      "outputs": [],
      "source": [
        "two_dimensional_array = np.array([[6, 5], [11, 7], [4, 8]])\n",
        "print(two_dimensional_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ED7eug9CvGR"
      },
      "source": [
        "To populate a matrix with all zeroes, call `np.zeros`. To populate a matrix with all ones, call `np.ones`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEy_pdBoROu3"
      },
      "source": [
        "## Populate arrays with sequences of numbers\n",
        "\n",
        "You can populate an array with a sequence of numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjHfYWhdQYtO"
      },
      "outputs": [],
      "source": [
        "sequence_of_integers = np.arange(5, 12)\n",
        "print(sequence_of_integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x3OoWrPWn8S"
      },
      "source": [
        "Notice that `np.arange` generates a sequence that includes the lower bound (5) but not the upper bound (12)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiqqxDBINAOY"
      },
      "source": [
        "## Populate arrays with random numbers\n",
        "\n",
        "NumPy provides various functions to populate matrices with random numbers across certain ranges. For example, `np.random.randint` generates random integers between a low and high value. The following call populates a 6-element vector with random integers between 50 and 100.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG8ao9CsNqw8"
      },
      "outputs": [],
      "source": [
        "random_integers_between_50_and_100 = np.random.randint(low=50, high=101, size=(6))\n",
        "print(random_integers_between_50_and_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSU7lMUcgRm3"
      },
      "source": [
        "Note that the highest generated integer `np.random.randint` is one less than the `high` argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQF6-Eg0ksqE"
      },
      "source": [
        "To create random floating-point values between 0.0 and 1.0, call `np.random.random`. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ny0eXZPk5Ax"
      },
      "outputs": [],
      "source": [
        "random_floats_between_0_and_1 = np.random.random([6])\n",
        "print(random_floats_between_0_and_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOdSjRlSEf6"
      },
      "source": [
        "## Mathematical Operations on NumPy Operands\n",
        "\n",
        "If you want to add or subtract two vectors or matrices, linear algebra requires that the two operands have the same dimensions. Furthermore, if you want to multiply two vectors or matrices, linear algebra imposes strict rules on the dimensional compatibility of operands. Fortunately, NumPy uses a trick called [**broadcasting**](https://developers.google.com/machine-learning/glossary/#broadcasting) to virtually expand the smaller operand to dimensions compatible for linear algebra. For example, the following operation uses broadcasting to add 2.0 to the value of every item in the vector created in the previous code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5E5S0wjRvQr"
      },
      "outputs": [],
      "source": [
        "random_floats_between_2_and_3 = random_floats_between_0_and_1 + 2.0\n",
        "print(random_floats_between_2_and_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6K_poVDPpAg"
      },
      "source": [
        "The following operation also relies on broadcasting to multiply each cell in a vector by 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYjvXmvFPoPB"
      },
      "outputs": [],
      "source": [
        "random_integers_between_150_and_300 = random_integers_between_50_and_100 * 3\n",
        "print(random_integers_between_150_and_300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfYVa8iQTaUL"
      },
      "source": [
        "## Task 1: Create a Linear Dataset\n",
        "\n",
        "Your goal is to create a simple dataset consisting of a single feature and a label as follows:\n",
        "\n",
        "1. Assign a sequence of 15 integers from 6 to 20 (inclusive) to a NumPy array named `feature`.\n",
        "2. Assign 15 values to a NumPy array named `label` such that:\n",
        "\n",
        "```\n",
        "   label = 3*(feature) + 4\n",
        "```\n",
        "For example, the first value for `label` could be:\n",
        "\n",
        "```\n",
        "  label = 3*(6) + 4 = 22\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK9UF2rUc3Y_"
      },
      "outputs": [],
      "source": [
        "feature = ? # write your code here\n",
        "print(feature)\n",
        "label = ?   # write your code here\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNsjGYRj87PB"
      },
      "source": [
        "## Task 2: Add Some Noise to the Dataset\n",
        "\n",
        "To make your dataset a little more realistic, insert a little random noise into each element of the `label` array you already created. To be more precise, modify each value assigned to `label` by adding a *different* random floating-point value between -2 and +2.\n",
        "\n",
        "Don't rely on broadcasting. Instead, create a `noise` array having the same dimension as `label`."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRlxJLxN_t82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-flFfs9r0q"
      },
      "outputs": [],
      "source": [
        "noise = ?    # write your code here\n",
        "print(noise)\n",
        "label = ?    # write your code here\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "jXm9zB8c_sE1"
      },
      "source": [
        "# Adapted from https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb#scrollTo=vO47lN3aDOAv, https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVZB64PiIv5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project"
      ],
      "metadata": {
        "id": "cy7c46mx_vwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learnings pipeline"
      ],
      "metadata": {
        "id": "dOyL3jRVIZL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier([\"I've been waiting for a HuggingFace course my whole life.\",\n",
        "           \"I hate this.\"])\n"
      ],
      "metadata": {
        "id": "IHocUleG_zZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\"This is a cource about the Transformers library\",\n",
        "           candidate_labels=[\"education\", \"politics\", \"business\"])"
      ],
      "metadata": {
        "id": "hDil29oBAepD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to cook a \")"
      ],
      "metadata": {
        "id": "hSH1d-mkA5B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how go for a\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "cDg39UYSBXXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ],
      "metadata": {
        "id": "IKBYum0aBoPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "zAz6vkPiCDgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answering = pipeline(\"question-answering\")\n",
        "question_answering(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ],
      "metadata": {
        "id": "eTfsrqNfCqb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(''' America has changed dramatically''')"
      ],
      "metadata": {
        "id": "0GQofKYZCzpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "id": "JPuCHa_1DxEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZnhTtY2I0Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similarity\n",
        "\n",
        "It measures the similarity between two non zero vectory by calculating the cosine of the angle between them. <br>\n",
        "Smaller distance indicates a higher similarity and larger distance indicates the lower similarity"
      ],
      "metadata": {
        "id": "ITCuz67OIfsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/asoria/awesome-chatgpt-prompts-embeddings/data/train-00000-of-00001.parquet\")"
      ],
      "metadata": {
        "id": "626GpbQOIpJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "8srR4_eZKzvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.embedding[0]"
      ],
      "metadata": {
        "id": "0HE7YiBtLBvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def array_cosine_similarity(arr1, arr2):\n",
        "  dot_product = np.dot(arr1, arr2)\n",
        "  norm_a = np.linalg.norm(arr1)\n",
        "  norm_b = np.linalg.norm(arr2)\n",
        "  return dot_product / (norm_a * norm_b)\n",
        "\n",
        "linux_embedding = df[df['act'] == 'Linux Terminal']['embedding'].iloc[0]\n",
        "\n",
        "df['similarity'] = df.apply(lambda row: array_cosine_similarity(row['embedding'], linux_embedding), axis=1)\n",
        "\n",
        "sorted_df = df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "print(sorted_df[['act', 'prompt', 'similarity']].head(3))"
      ],
      "metadata": {
        "id": "9__gWVqfLR9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project\n"
      ],
      "metadata": {
        "id": "-lpDFtT1NkNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import transformers as pipeline\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/philschmid/amazon-product-descriptions-vlm/data/train-00000-of-00001.parquet\")\n",
        "df"
      ],
      "metadata": {
        "id": "722isL0iZSKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "510aba0d"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5503db1"
      },
      "source": [
        "### Subtask: Generate and add embeddings\n",
        "\n",
        "Generate embeddings for the `description` column using the loaded model and add them as a new column to the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "152e7790"
      },
      "source": [
        "# Description embedding\n",
        "embeddings = model.encode(df['description'].tolist())\n",
        "\n",
        "df['description_embedding'] = list(embeddings)\n",
        "\n",
        "display(df['description_embedding'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_parquet(\"hf://datasets/philschmid/amazon-product-descriptions-vlm/data/train-00000-of-00001.parquet\")\n",
        "df\n",
        "\n",
        "# Sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Embeddings for descriptions\n",
        "embeddings = model.encode(df['description'].tolist(), convert_to_tensor=True)\n",
        "df['description_embedding'] = embeddings.tolist()\n",
        "\n",
        "for i, emb in enumerate(df['description_embedding'].head()):\n",
        "     print(f\"Embedding for description {i}: {emb[:10]}...\")\n",
        "\n",
        "# Function to find similar descriptions\n",
        "def find_similar_descriptions(input_text, dataframe, model, top_k=1):\n",
        "\n",
        "    # Embedding for input text\n",
        "    input_embedding = model.encode(input_text, convert_to_tensor=True)\n",
        "\n",
        "    # Cosine similarity\n",
        "    cosine_scores = util.cos_sim(input_embedding, torch.tensor(dataframe['description_embedding'].tolist()))[0]\n",
        "\n",
        "    # Top similar descritions\n",
        "    top_results = torch.topk(cosine_scores, k=top_k)\n",
        "\n",
        "    similar_descriptions = []\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        similar_descriptions.append({\n",
        "            'description': dataframe.loc[idx.item(), 'description'],\n",
        "            'similarity_score': score.item()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(similar_descriptions)\n",
        "\n",
        "# 5. Example Usage\n",
        "    user_input = \"Everlast orofessional hand wraps.\"\n",
        "    print(f\"\\nFinding descriptions similar to: '{user_input}'\")\n",
        "    most_similar = find_similar_descriptions(user_input, df, model, top_k=3)\n",
        "    print(most_similar)\n",
        "\n",
        "    user_input_2 = \"This item is bad quality and easily broken.\"\n",
        "    print(f\"\\nFinding descriptions similar to: '{user_input_2}'\")\n",
        "    most_similar_2 = find_similar_descriptions(user_input_2, df, model, top_k=2)\n",
        "    print(most_similar_2)"
      ],
      "metadata": {
        "id": "KUExr4ilnFl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch # Import torch for handling tensors, if needed by sentence-transformers\n",
        "\n",
        "# 1. Load the dataset\n",
        "# Ensure you have access to the dataset. 'hf://datasets/' implies a Hugging Face dataset.\n",
        "# If you're running this locally, make sure the dataset is downloaded or accessible.\n",
        "try:\n",
        "    df = pd.read_parquet(\"hf://datasets/philschmid/amazon-product-descriptions-vlm/data/train-00000-of-00001.parquet\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure you have internet connectivity or the dataset is available locally.\")\n",
        "    # Create a dummy DataFrame for demonstration if the real one can't be loaded\n",
        "    data = {'description': [\n",
        "        \"This is a fantastic product, highly recommend it for daily use.\",\n",
        "        \"A very durable and long-lasting item, perfect for outdoor activities.\",\n",
        "        \"Not what I expected, quite flimsy and broke quickly.\",\n",
        "        \"Excellent quality and great value for money, will buy again.\",\n",
        "        \"Good for the price, but could be more robust.\"\n",
        "    ]}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using a dummy DataFrame for demonstration.\")\n",
        "\n",
        "\n",
        "# 2. Initialize the Sentence Transformer Model\n",
        "# 'all-MiniLM-L6-v2' is a good general-purpose model for sentence embeddings.\n",
        "# You can explore other models on the Hugging Face Model Hub based on your needs.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Sentence Transformer model loaded.\")\n",
        "\n",
        "# 3. Generate embeddings for all descriptions\n",
        "print(\"Generating embeddings for descriptions...\")\n",
        "embeddings = model.encode(df['description'].tolist(), convert_to_tensor=True) # convert_to_tensor=True for GPU optimization if available\n",
        "df['description_embedding'] = embeddings.tolist() # Store as list if you prefer, or keep as tensor if you're doing more tensor operations\n",
        "\n",
        "print(\"Embeddings generated and added to DataFrame.\")\n",
        "# Display the first few embeddings (optional)\n",
        "# for i, emb in enumerate(df['description_embedding'].head()):\n",
        "#     print(f\"Embedding for description {i}: {emb[:10]}...\") # Print first 10 dimensions\n",
        "\n",
        "# 4. Function to find similar descriptions\n",
        "def find_similar_descriptions(input_text, dataframe, model, top_k=1):\n",
        "    \"\"\"\n",
        "    Finds descriptions in the DataFrame most similar to the input text.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): The text to compare against the descriptions.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing 'description' and 'description_embedding' columns.\n",
        "        model (SentenceTransformer): The pre-trained sentence transformer model.\n",
        "        top_k (int): The number of most similar descriptions to return.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the top_k most similar descriptions and their similarity scores.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the input text\n",
        "    input_embedding = model.encode(input_text, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities between input_embedding and all description_embeddings\n",
        "    # Use util.cos_sim for efficiency with SentenceTransformer embeddings\n",
        "    cosine_scores = util.cos_sim(input_embedding, torch.tensor(dataframe['description_embedding'].tolist()))[0]\n",
        "\n",
        "    # Get the top_k indices with the highest similarity scores\n",
        "    top_results = torch.topk(cosine_scores, k=top_k)\n",
        "\n",
        "    similar_descriptions = []\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        similar_descriptions.append({\n",
        "            'description': dataframe.loc[idx.item(), 'description'],\n",
        "            'similarity_score': score.item()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(similar_descriptions)\n",
        "\n",
        "# 5. Example Usage\n",
        "if not df.empty:\n",
        "    user_input = \"Looking for a very durable product that lasts long.\"\n",
        "    print(f\"\\nFinding descriptions similar to: '{user_input}'\")\n",
        "    most_similar = find_similar_descriptions(user_input, df, model, top_k=3)\n",
        "    print(most_similar)\n",
        "\n",
        "    user_input_2 = \"This item is bad quality and easily broken.\"\n",
        "    print(f\"\\nFinding descriptions similar to: '{user_input_2}'\")\n",
        "    most_similar_2 = find_similar_descriptions(user_input_2, df, model, top_k=2)\n",
        "    print(most_similar_2)\n",
        "else:\n",
        "    print(\"\\nDataFrame is empty, cannot perform similarity search.\")"
      ],
      "metadata": {
        "id": "NOxas52JqaSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NumPy UltraQuick Tutorial",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}